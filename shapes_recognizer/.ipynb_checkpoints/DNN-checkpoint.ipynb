{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\pdv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\ipdv}[2]{\\partial #1/\\partial #2}\n",
    "\\newcommand{\\dd}[1]{\\,\\textit{d}#1\\,}\n",
    "\\newcommand{\\softmax}[1]{\\Softmax\\left(#1\\right)}\n",
    "\\newcommand{\\smax}[1]{\\Smax\\left(#1\\right)}\n",
    "\\newcommand{\\exp}[1]{e^{#1}}\n",
    "\\newcommand{\\grad}{\\nabla}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\idm}{\\mathbb{1}}  % \\idm identity matrix\n",
    "\\DeclareMathOperator{\\Softmax}{softmax}\n",
    "\\DeclareMathOperator{\\Smax}{smax}\n",
    "\\DeclareMathOperator{\\relu}{ReLU}\n",
    "\\DeclareMathOperator{\\mat}{Mat}\n",
    "\\DeclareMathOperator{\\GL}{GL}\n",
    "\\DeclareMathOperator{\\SL}{SL}\n",
    "\\DeclareMathOperator{\\diag}{diag}\n",
    "\\DeclareMathOperator{\\sgn}{sgn}\n",
    "\\DeclareMathOperator{\\lexp}{exp}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Lets-prepare-our-data\" data-toc-modified-id=\"Lets-prepare-our-data-1\">Lets prepare our data</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-example-by-column\" data-toc-modified-id=\"One-example-by-column-1.1\">One example by column</a></span></li><li><span><a href=\"#Flattening-images\" data-toc-modified-id=\"Flattening-images-1.2\">Flattening images</a></span></li><li><span><a href=\"#Standardize-data-to-have-feature-values-between-0-and-1\" data-toc-modified-id=\"Standardize-data-to-have-feature-values-between-0-and-1-1.3\">Standardize data to have feature values between 0 and 1</a></span></li><li><span><a href=\"#Check-the-shapes\" data-toc-modified-id=\"Check-the-shapes-1.4\">Check the shapes</a></span></li><li><span><a href=\"#One-hot-encoding-the-labels-vectors\" data-toc-modified-id=\"One-hot-encoding-the-labels-vectors-1.5\">One hot encoding the labels vectors</a></span></li><li><span><a href=\"#Final-check-of-shapes\" data-toc-modified-id=\"Final-check-of-shapes-1.6\">Final check of shapes</a></span></li></ul></li><li><span><a href=\"#Defining-the-model,-i.e.-the-NN-architecture\" data-toc-modified-id=\"Defining-the-model,-i.e.-the-NN-architecture-2\">Defining the model, i.e. the NN architecture</a></span></li><li><span><a href=\"#Initializing-the-parameters\" data-toc-modified-id=\"Initializing-the-parameters-3\">Initializing the parameters</a></span></li><li><span><a href=\"#Defining-activation-functions\" data-toc-modified-id=\"Defining-activation-functions-4\">Defining activation functions</a></span></li><li><span><a href=\"#Forward-Propagation\" data-toc-modified-id=\"Forward-Propagation-5\">Forward Propagation</a></span></li><li><span><a href=\"#Computing-the-current-Cost\" data-toc-modified-id=\"Computing-the-current-Cost-6\">Computing the current Cost</a></span></li><li><span><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-7\">Backpropagation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Network\n",
    "<hr>\n",
    "\n",
    "A DNN can be decomposed as the following sequence of operations:\n",
    "* Input Data\n",
    "* Forward Propagation to obtain an Output Data\n",
    "* Evaluate Output Data (compute current Cost)\n",
    "* Given current Cost, do Back-propagation to update weights\n",
    "* Repeat from beggining with updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run shapesdata.ipynb\n",
    "\n",
    "# Loading the data (circle, square and triangle drawings)\n",
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig, _, _ = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a ['triangle']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a drawing and its label\n",
    "print_img(train_x_orig, train_y_orig, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_orig: (270, 28, 28, 3)\n",
      "train_y_orig: (270, 1)\n",
      "test_x_orig: (30, 28, 28, 3)\n",
      "test_y_orig: (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# Lets see what are the shapes of our variables\n",
    "print(\"train_x_orig: {}\".format(train_x_orig.shape))\n",
    "print(\"train_y_orig: {}\".format(train_y_orig.shape))\n",
    "print(\"test_x_orig: {}\".format(test_x_orig.shape))\n",
    "print(\"test_y_orig: {}\".format(test_y_orig.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets prepare our data\n",
    "\n",
    "### One example by column\n",
    "We can arrange our example either by rows or by columns. Here we choose to arrange them by columns.\n",
    "\n",
    "### Flattening images\n",
    "First, we need to flatten our images, since they are actually arrays and we want them to be vectors. Our **train_x_orig** and **test_x_orig** variables are arrays with shape **(210, 28, 28, 3)**, where the first number stands for the number of examples we have in the set and the remaining three number are a single image array. We want them to be arrays of shape **(28\\*28\\*3, 210)**, so we use numpy's reshape method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data to have feature values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x_flatten/255\n",
    "test_x = test_x_flatten/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (2352, 270)\n",
      "test_x: (2352, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"test_x: {}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding the labels vectors\n",
    "Our **train_y_orig** and **test_y_orig** variables are arrays with shape **(210, 1)**, i.e. column vectors, and to each example there is an associate class indicated by a string, i.e. 'circle'. However, we want this classes to be indicated by numbers and the best way to do that is by a process called **one hot encoding**:\n",
    "* We define a vector whose each component corresponds to a class, and we indicate that our example belongs to a certain class by filling this vector with zeros except for the corresponding class component, which we fill with 1.\n",
    "* To each example, then, we associate one of this vectors.\n",
    "\n",
    "After one hot encoding train_y_orig and test_y_orig we should have labels vectors **train_y** and **test_y** of shape **(3, 210)** and **(3, 90)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencode(y):\n",
    "    \n",
    "    classes = np.unique(y)\n",
    "    \n",
    "    one_hot_y = np.zeros((y.shape[0], len(classes)))\n",
    "    \n",
    "    for i, item in enumerate(y):\n",
    "#         print(i, item)\n",
    "        one_hot_y[i] = item == classes\n",
    "\n",
    "    one_hot_y = one_hot_y.T\n",
    "    \n",
    "    return one_hot_y\n",
    "\n",
    "train_y = onehotencode(train_y_orig)\n",
    "test_y = onehotencode(test_y_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final check of shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (2352, 270)\n",
      "train_y: (3, 270)\n",
      "test_x: (2352, 30)\n",
      "test_y: (3, 30)\n"
     ]
    }
   ],
   "source": [
    "# Lets see what are the new shapes of our variables\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x: {}\".format(test_x.shape))\n",
    "print(\"test_y: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "                  including the dimension of the input and the output\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    r = np.maximum(0,x) + 0.01*np.minimum(0,x)  \n",
    "    return r\n",
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    x_exp = np.exp(x)\n",
    "    \n",
    "    x_sum = np.sum(x_exp, axis = 0, keepdims = True)\n",
    "    \n",
    "    s = x_exp/x_sum\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "We know that when going from the layer $l-1$ to the layer $l$ we do the following:\n",
    "\\begin{equation}\n",
    "    Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]},\n",
    "\\end{equation}\n",
    "then\n",
    "\\begin{equation}\n",
    "    A^{[l]} = g^{[l]}(Z^{[l]}),\n",
    "\\end{equation}\n",
    "so let's write a code to perform this steps, bearing in mind that we will use them inside the main iteration loop.\n",
    "\n",
    "* Lets denote $A^{[l-1]}$ by **A_prev**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def step_forward(A_prev, W, b, activation_function = \"relu\"):\n",
    "    \"\"\"\n",
    "    Implement a layer's forward propagation step.\n",
    "\n",
    "    Arguments:\n",
    "        A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "        W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "        b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "        A -- activation \n",
    "        cache -- a python dictionary containing \"A_prev\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute pre-activation output\n",
    "    Z = W @ A_prev + b \n",
    "    \n",
    "    # Check the dimensions of Z\n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1])) \n",
    "    \n",
    "    if activation_function == 'relu':\n",
    "        A = relu(Z)\n",
    "        \n",
    "    if activation_function == 'softmax':\n",
    "        A = softmax(Z)\n",
    "        \n",
    "    assert (A.shape == Z.shape) # This line checks the dimensions of A, which should be the same as of Z\n",
    "\n",
    "    cache = (A_prev, W, b, Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- parameters dictionary\n",
    "    hidden_activ_func -- relu, tanh\n",
    "    last_activ_func -- softmax, sigmoid\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation output\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    \n",
    "    A = X\n",
    "    L = len(parameters) // 2     # number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L): # l = 1,..., L-1\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\"+str(l)]\n",
    "        b = parameters[\"b\"+str(l)]\n",
    "        \n",
    "        A, cache = step_forward(A_prev, W, b, 'relu')\n",
    "        \n",
    "        caches.append(cache)\n",
    "        \n",
    "    A_prev = A\n",
    "    W = parameters[\"W\"+str(L)]\n",
    "    b = parameters[\"b\"+str(L)]\n",
    "    \n",
    "    AL, cache = step_forward(A_prev, W, b, 'softmax')\n",
    "        \n",
    "    caches.append(cache)\n",
    "                \n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the current Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute loss from AL and y.\n",
    "    R = Y*np.log(AL)\n",
    "    loss = (-1)*np.sum(R, axis = 0, keepdims = True) # Computes the loss for each example\n",
    "        \n",
    "    assert(loss.shape == (1,Y.shape[1]))\n",
    "    \n",
    "    cost = (1/M)*np.sum(loss)\n",
    "    \n",
    "    cost = np.squeeze(cost) # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(z):\n",
    "    x = np.zeros(z.shape)\n",
    "    x[z > 0] = 1\n",
    "    x[z <= 0] = 0.01\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_step(dA, A_prev, W, b, Z, activation_function = \"relu\"):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "        \n",
    "    if activation_function == 'relu':\n",
    "        dZ = np.multiply(dA, relu_derivative(Z))\n",
    "        \n",
    "    if activation_function == 'softmax':\n",
    "        dZ = np.multiply(dA,softmax(Z)) - np.diag(dA.T @ softmax(Z))*softmax(Z)\n",
    "        \n",
    "    dW = (dZ @ A_prev.T)\n",
    "    \n",
    "    db = np.sum(dZ, axis = 1, keepdims = True)\n",
    "    \n",
    "    dA_prev = W.T @ dZ\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    M = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = (1/M)*(-Y/AL)\n",
    "        \n",
    "    dA = dAL\n",
    "    \n",
    "    A_prev, W, b, Z = caches[L-1]\n",
    "    \n",
    "    dA_prev, dW, db = backward_step(dA, A_prev, W, b, Z, 'softmax')\n",
    "    \n",
    "    grads[\"dA\" + str(L-1)] = dA_prev\n",
    "    grads[\"dW\" + str(L)] = dW\n",
    "    grads[\"db\" + str(L)] = db\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(0, L-1)):   # Note that the first value \"l\" takes is L-2 \n",
    "        \n",
    "        dA = grads[\"dA\" + str(l+1)]  # Note that the first index used is \"l+1\" = L-1, whish follows the L we already used\n",
    "        A_prev, W, b, Z = caches[l]\n",
    "    \n",
    "        dA_prev, dW, db = backward_step(dA, A_prev, W, b, Z, 'relu')\n",
    "\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l+1)] = dW\n",
    "        grads[\"db\" + str(l+1)] = db\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of layers in the neural network\n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "            \n",
    "            \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(X,\n",
    "               Y,\n",
    "               dev_x, \n",
    "               dev_y, \n",
    "               layers_dims, \n",
    "               learning_rate = 0.01, \n",
    "               num_iterations = 3000, \n",
    "               print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    train_errors = []                         # keep track of train error\n",
    "    test_errors = []                          # keep track of train error\n",
    "    \n",
    "    # Parameters initialization. (â‰ˆ 1 line of code)\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, caches = forward_propagation(X, parameters, hidden_activ_func, last_activ_func)\n",
    "        \n",
    "        dev_AL, _ = forward_propagation(dev_x, parameters, hidden_activ_func, last_activ_func)\n",
    "        \n",
    "        # Compute cost.\n",
    "        train_error = compute_cost(AL, Y, loss_function)\n",
    "        \n",
    "        test_error = compute_cost(dev_AL, dev_y, loss_function)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, train_error))\n",
    "        if print_cost and i % 10 == 0:\n",
    "            train_errors.append(train_error)\n",
    "        if print_cost and i % 10 == 0:\n",
    "            test_errors.append(test_error)\n",
    "        \n",
    "    # plot the cost\n",
    "#     plt.plot(np.squeeze(train_errors))\n",
    "    plt.plot(np.squeeze(test_errors))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \n",
    "    Yhat, _ = forward_propagation(X, parameters)\n",
    "    \n",
    "    pred = np.zeros(Yhat.shape).T\n",
    "    \n",
    "    for m in range(Yhat.shape[1]):\n",
    "        pred[m][np.argmax(Yhat.T[m], axis = 0)] = 1\n",
    "    return pred.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_right(train_y, train_pred):\n",
    "    \n",
    "    prod = train_y*train_pred\n",
    "        \n",
    "    ans = np.logical_and(*(train_pred == train_y))\n",
    "    \n",
    "    ans = ans[np.newaxis, :]\n",
    "    \n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(X, y_true, parameters):\n",
    "    pred = predict(X, parameters)\n",
    "    isright = check_right(y_true, pred)\n",
    "    a = np.unique(isright, return_counts=True)\n",
    "    if len(a[1]) == 2:\n",
    "        percen_wrong = a[1][0]/np.sum(a[1])\n",
    "        percen_right = a[1][1]/np.sum(a[1])\n",
    "    else:\n",
    "        percen_right = 1\n",
    "    return percen_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.111940\n",
      "Cost after iteration 1000: 0.998577\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1250 iteration, the model achieved a 53.70370370370371% accuracy on the train set.\n",
      "With 1250 iteration, the model achieved a 36.666666666666664% accuracy on the test set.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "M = train_x.shape[1]                     # Number of examples\n",
    "\n",
    "n_x = train_x.shape[0]                   # Number of features\n",
    "\n",
    "C = 3                                    # Number of classes\n",
    "\n",
    "hidden_layers = [50, 50, 50, 50]         # Number of hidden layers and their respective size\n",
    "\n",
    "layer_dims = [n_x, *hidden_layers, C]    # Neural Network Architecture\n",
    "\n",
    "num_iter = 1000\n",
    "\n",
    "trained_parameters = deep_model(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    test_x,\n",
    "    test_y,\n",
    "    layer_dims,\n",
    "    learning_rate = 0.003, \n",
    "    num_iterations = num_iter, \n",
    "    print_cost=True)\n",
    "\n",
    "acc_train = get_score(train_x, train_y, trained_parameters)\n",
    "acc_test = get_score(test_x, test_y, trained_parameters)\n",
    "\n",
    "print(\"With {} iteration, the model achieved a {}% accuracy on the train set.\".format(num_iter, acc_train*100))\n",
    "print(\"With {} iteration, the model achieved a {}% accuracy on the test set.\".format(num_iter, acc_test*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
