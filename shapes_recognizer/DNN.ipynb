{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\pdv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\ipdv}[2]{\\partial #1/\\partial #2}\n",
    "\\newcommand{\\dd}[1]{\\,\\textit{d}#1\\,}\n",
    "\\newcommand{\\softmax}[1]{\\Softmax\\left(#1\\right)}\n",
    "\\newcommand{\\smax}[1]{\\Smax\\left(#1\\right)}\n",
    "\\newcommand{\\exp}[1]{e^{#1}}\n",
    "\\newcommand{\\grad}{\\nabla}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\idm}{\\mathbb{1}}  % \\idm identity matrix\n",
    "\\DeclareMathOperator{\\Softmax}{softmax}\n",
    "\\DeclareMathOperator{\\Smax}{smax}\n",
    "\\DeclareMathOperator{\\relu}{ReLU}\n",
    "\\DeclareMathOperator{\\mat}{Mat}\n",
    "\\DeclareMathOperator{\\GL}{GL}\n",
    "\\DeclareMathOperator{\\SL}{SL}\n",
    "\\DeclareMathOperator{\\diag}{diag}\n",
    "\\DeclareMathOperator{\\sgn}{sgn}\n",
    "\\DeclareMathOperator{\\lexp}{exp}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Lets-prepare-our-data\" data-toc-modified-id=\"Lets-prepare-our-data-1\">Lets prepare our data</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-example-by-column\" data-toc-modified-id=\"One-example-by-column-1.1\">One example by column</a></span></li><li><span><a href=\"#Flattening-images\" data-toc-modified-id=\"Flattening-images-1.2\">Flattening images</a></span></li><li><span><a href=\"#Standardize-data-to-have-feature-values-between-0-and-1\" data-toc-modified-id=\"Standardize-data-to-have-feature-values-between-0-and-1-1.3\">Standardize data to have feature values between 0 and 1</a></span></li><li><span><a href=\"#Check-the-shapes\" data-toc-modified-id=\"Check-the-shapes-1.4\">Check the shapes</a></span></li><li><span><a href=\"#One-hot-encoding-the-labels-vectors\" data-toc-modified-id=\"One-hot-encoding-the-labels-vectors-1.5\">One hot encoding the labels vectors</a></span></li><li><span><a href=\"#Final-check-of-shapes\" data-toc-modified-id=\"Final-check-of-shapes-1.6\">Final check of shapes</a></span></li></ul></li><li><span><a href=\"#Defining-the-model,-i.e.-the-NN-architecture\" data-toc-modified-id=\"Defining-the-model,-i.e.-the-NN-architecture-2\">Defining the model, i.e. the NN architecture</a></span></li><li><span><a href=\"#Initializing-the-parameters\" data-toc-modified-id=\"Initializing-the-parameters-3\">Initializing the parameters</a></span></li><li><span><a href=\"#Defining-activation-functions\" data-toc-modified-id=\"Defining-activation-functions-4\">Defining activation functions</a></span></li><li><span><a href=\"#Forward-Propagation\" data-toc-modified-id=\"Forward-Propagation-5\">Forward Propagation</a></span></li><li><span><a href=\"#Computing-the-current-Cost\" data-toc-modified-id=\"Computing-the-current-Cost-6\">Computing the current Cost</a></span></li><li><span><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-7\">Backpropagation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Network\n",
    "<hr>\n",
    "\n",
    "A DNN can be decomposed as the following sequence of operations:\n",
    "* Input Data\n",
    "* Forward Propagation to obtain an Output Data\n",
    "* Evaluate Output Data (compute current Cost)\n",
    "* Given current Cost, do Back-propagation to update weights\n",
    "* Repeat from beggining with updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run shapesdata.ipynb\n",
    "\n",
    "# Loading the data (circle, square and triangle drawings)\n",
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig, _, _ = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a ['triangle']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a drawing and its label\n",
    "print_img(train_x_orig, train_y_orig, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_orig: (270, 28, 28, 3)\n",
      "train_y_orig: (270, 1)\n",
      "test_x_orig: (30, 28, 28, 3)\n",
      "test_y_orig: (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# Lets see what are the shapes of our variables\n",
    "print(\"train_x_orig: {}\".format(train_x_orig.shape))\n",
    "print(\"train_y_orig: {}\".format(train_y_orig.shape))\n",
    "print(\"test_x_orig: {}\".format(test_x_orig.shape))\n",
    "print(\"test_y_orig: {}\".format(test_y_orig.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets prepare our data\n",
    "\n",
    "### One example by column\n",
    "We can arrange our example either by rows or by columns. Here we choose to arrange them by columns.\n",
    "\n",
    "### Flattening images\n",
    "First, we need to flatten our images, since they are actually arrays and we want them to be vectors. Our **train_x_orig** and **test_x_orig** variables are arrays with shape **(210, 28, 28, 3)**, where the first number stands for the number of examples we have in the set and the remaining three number are a single image array. We want them to be arrays of shape **(28\\*28\\*3, 210)**, so we use numpy's reshape method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data to have feature values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x_flatten/255\n",
    "test_x = test_x_flatten/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (2352, 270)\n",
      "test_x: (2352, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"test_x: {}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding the labels vectors\n",
    "Our **train_y_orig** and **test_y_orig** variables are arrays with shape **(210, 1)**, i.e. column vectors, and to each example there is an associate class indicated by a string, i.e. 'circle'. However, we want this classes to be indicated by numbers and the best way to do that is by a process called **one hot encoding**:\n",
    "* We define a vector whose each component corresponds to a class, and we indicate that our example belongs to a certain class by filling this vector with zeros except for the corresponding class component, which we fill with 1.\n",
    "* To each example, then, we associate one of this vectors.\n",
    "\n",
    "After one hot encoding train_y_orig and test_y_orig we should have labels vectors **train_y** and **test_y** of shape **(3, 210)** and **(3, 90)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencode(y):\n",
    "    \n",
    "    classes = np.unique(y)\n",
    "    \n",
    "    one_hot_y = np.zeros((y.shape[0], len(classes)))\n",
    "    \n",
    "    for i, item in enumerate(y):\n",
    "#         print(i, item)\n",
    "        one_hot_y[i] = item == classes\n",
    "\n",
    "    one_hot_y = one_hot_y.T\n",
    "    \n",
    "    return one_hot_y\n",
    "\n",
    "train_y = onehotencode(train_y_orig)\n",
    "test_y = onehotencode(test_y_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final check of shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (2352, 270)\n",
      "train_y: (3, 270)\n",
      "test_x: (2352, 30)\n",
      "test_y: (3, 30)\n"
     ]
    }
   ],
   "source": [
    "# Lets see what are the new shapes of our variables\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x: {}\".format(test_x.shape))\n",
    "print(\"test_y: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "                  including the dimension of the input and the output\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "\n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    r = np.maximum(0,x) + 0.01*np.minimum(0,x)  \n",
    "    return r\n",
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    x_exp = np.exp(x)\n",
    "    \n",
    "    x_sum = np.sum(x_exp, axis = 0, keepdims = True)\n",
    "    \n",
    "    s = x_exp/x_sum\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "We know that when going from the layer $l-1$ to the layer $l$ we do the following:\n",
    "\\begin{equation}\n",
    "    Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]},\n",
    "\\end{equation}\n",
    "then\n",
    "\\begin{equation}\n",
    "    A^{[l]} = g^{[l]}(Z^{[l]}),\n",
    "\\end{equation}\n",
    "so let's write a code to perform this steps, bearing in mind that we will use them inside the main iteration loop.\n",
    "\n",
    "* Lets denote $A^{[l-1]}$ by **A_prev**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def step_forward(A_prev, W, b, activation_function = \"relu\"):\n",
    "    \"\"\"\n",
    "    Implement a layer's forward propagation step.\n",
    "\n",
    "    Arguments:\n",
    "        A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "        W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "        b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "        A -- activation \n",
    "        cache -- a python dictionary containing \"A_prev\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute pre-activation output\n",
    "    Z = W @ A_prev + b \n",
    "    \n",
    "    # Check the dimensions of Z\n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1])) \n",
    "    \n",
    "    if activation_function == 'relu':\n",
    "        A = relu(Z)\n",
    "        \n",
    "    if activation_function == 'softmax':\n",
    "        A = softmax(Z)\n",
    "        \n",
    "    assert (A.shape == Z.shape) # This line checks the dimensions of A, which should be the same as of Z\n",
    "\n",
    "    cache = (A_prev, W, b, Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- parameters dictionary\n",
    "    hidden_activ_func -- relu, tanh\n",
    "    last_activ_func -- softmax, sigmoid\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation output\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    \n",
    "    A = X\n",
    "    L = len(parameters) // 2     # number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L): # l = 1,..., L-1\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\"+str(l)]\n",
    "        b = parameters[\"b\"+str(l)]\n",
    "        \n",
    "        A, cache = step_forward(A_prev, W, b, 'relu')\n",
    "        \n",
    "        caches.append(cache)\n",
    "        \n",
    "    A_prev = A\n",
    "    W = parameters[\"W\"+str(L)]\n",
    "    b = parameters[\"b\"+str(L)]\n",
    "    \n",
    "    AL, cache = step_forward(A_prev, W, b, 'softmax')\n",
    "        \n",
    "    caches.append(cache)\n",
    "                \n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the current Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute loss from AL and y.\n",
    "    R = Y*np.log(AL)\n",
    "    loss = (-1)*np.sum(R, axis = 0, keepdims = True) # Computes the loss for each example\n",
    "        \n",
    "    assert(loss.shape == (1,Y.shape[1]))\n",
    "    \n",
    "    cost = (1/M)*np.sum(loss)\n",
    "    \n",
    "    cost = np.squeeze(cost) # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(z):\n",
    "    x = np.zeros(z.shape)\n",
    "    x[z > 0] = 1\n",
    "    x[z <= 0] = 0.01\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_step(dA, A_prev, W, b, Z, activation_function = \"relu\"):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "        \n",
    "    if activation_function == 'relu':\n",
    "        dZ = np.multiply(dA, relu_derivative(Z))\n",
    "        \n",
    "    if activation_function == 'softmax':\n",
    "        dZ = np.multiply(dA,softmax(Z)) - np.diag(dA.T @ softmax(Z))*softmax(Z)\n",
    "        \n",
    "    dW = (dZ @ A_prev.T)\n",
    "    \n",
    "    db = np.sum(dZ, axis = 1, keepdims = True)\n",
    "    \n",
    "    dA_prev = W.T @ dZ\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    M = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = (1/M)*(-Y/AL)\n",
    "        \n",
    "    dA = dAL\n",
    "    \n",
    "    A_prev, W, b, Z = caches[L-1]\n",
    "    \n",
    "    dA_prev, dW, db = backward_step(dA, A_prev, W, b, Z, 'softmax')\n",
    "    \n",
    "    grads[\"dA\" + str(L-1)] = dA_prev\n",
    "    grads[\"dW\" + str(L)] = dW\n",
    "    grads[\"db\" + str(L)] = db\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(0, L-1)):   # Note that the first value \"l\" takes is L-2 \n",
    "        \n",
    "        dA = grads[\"dA\" + str(l+1)]  # Note that the first index used is \"l+1\" = L-1, whish follows the L we already used\n",
    "        A_prev, W, b, Z = caches[l]\n",
    "    \n",
    "        dA_prev, dW, db = backward_step(dA, A_prev, W, b, Z, 'relu')\n",
    "\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l+1)] = dW\n",
    "        grads[\"db\" + str(l+1)] = db\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of layers in the neural network\n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "            \n",
    "            \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(X,\n",
    "               Y,\n",
    "               dev_x, \n",
    "               dev_y, \n",
    "               layers_dims, \n",
    "               learning_rate = 0.01, \n",
    "               num_iterations = 3000, \n",
    "               print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    train_errors = []                         # keep track of train error\n",
    "    test_errors = []                          # keep track of train error\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "        dev_AL, _ = forward_propagation(dev_x, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        train_error = compute_cost(AL, Y)\n",
    "        \n",
    "        test_error = compute_cost(dev_AL, dev_y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, train_error))\n",
    "        if print_cost and i % 10 == 0:\n",
    "            train_errors.append(train_error)\n",
    "        if print_cost and i % 10 == 0:\n",
    "            test_errors.append(test_error)\n",
    "        \n",
    "    # plot the cost\n",
    "#     plt.plot(np.squeeze(train_errors))\n",
    "    plt.plot(np.squeeze(test_errors))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \n",
    "    Yhat, _ = forward_propagation(X, parameters)\n",
    "    \n",
    "    pred = np.zeros(Yhat.shape).T\n",
    "    \n",
    "    for m in range(Yhat.shape[1]):\n",
    "        pred[m][np.argmax(Yhat.T[m], axis = 0)] = 1\n",
    "    return pred.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_right(train_y, train_pred):\n",
    "    \n",
    "    prod = train_y*train_pred\n",
    "        \n",
    "    ans = np.logical_and(*(train_pred == train_y))\n",
    "    \n",
    "    ans = ans[np.newaxis, :]\n",
    "    \n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(X, y_true, parameters):\n",
    "    pred = predict(X, parameters)\n",
    "    isright = check_right(y_true, pred)\n",
    "    a = np.unique(isright, return_counts=True)\n",
    "    if len(a[1]) == 2:\n",
    "        percen_wrong = a[1][0]/np.sum(a[1])\n",
    "        percen_right = a[1][1]/np.sum(a[1])\n",
    "    else:\n",
    "        percen_right = 1\n",
    "    return percen_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.216688\n",
      "Cost after iteration 100: 1.053543\n",
      "Cost after iteration 200: 1.028952\n",
      "Cost after iteration 300: 1.003631\n",
      "Cost after iteration 400: 0.978725\n",
      "Cost after iteration 500: 0.953525\n",
      "Cost after iteration 600: 0.927945\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 700 iteration, the model achieved a 64.44444444444444% accuracy on the train set.\n",
      "With 700 iteration, the model achieved a 53.333333333333336% accuracy on the test set.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "M = train_x.shape[1]                     # Number of examples\n",
    "\n",
    "n_x = train_x.shape[0]                   # Number of features\n",
    "\n",
    "C = 3                                    # Number of classes\n",
    "\n",
    "hidden_layers = [50, 50, 50, 50]         # Number of hidden layers and their respective size\n",
    "\n",
    "layer_dims = [n_x, *hidden_layers, C]    # Neural Network Architecture\n",
    "\n",
    "num_iter = 700\n",
    "\n",
    "trained_parameters = deep_model(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    test_x,\n",
    "    test_y,\n",
    "    layer_dims,\n",
    "    learning_rate = 0.001, \n",
    "    num_iterations = num_iter, \n",
    "    print_cost=True)\n",
    "\n",
    "acc_train = get_score(train_x, train_y, trained_parameters)\n",
    "acc_test = get_score(test_x, test_y, trained_parameters)\n",
    "\n",
    "print(\"With {} iteration, the model achieved a {}% accuracy on the train set.\".format(num_iter, acc_train*100))\n",
    "print(\"With {} iteration, the model achieved a {}% accuracy on the test set.\".format(num_iter, acc_test*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
