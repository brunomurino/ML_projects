{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Lets-prepare-our-data\" data-toc-modified-id=\"Lets-prepare-our-data-1\">Lets prepare our data</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-example-by-column\" data-toc-modified-id=\"One-example-by-column-1.1\">One example by column</a></span></li><li><span><a href=\"#Flattening-images\" data-toc-modified-id=\"Flattening-images-1.2\">Flattening images</a></span></li><li><span><a href=\"#Standardize-data-to-have-feature-values-between-0-and-1\" data-toc-modified-id=\"Standardize-data-to-have-feature-values-between-0-and-1-1.3\">Standardize data to have feature values between 0 and 1</a></span></li><li><span><a href=\"#Check-the-shapes\" data-toc-modified-id=\"Check-the-shapes-1.4\">Check the shapes</a></span></li><li><span><a href=\"#One-hot-encoding-the-labels-vectors\" data-toc-modified-id=\"One-hot-encoding-the-labels-vectors-1.5\">One hot encoding the labels vectors</a></span></li><li><span><a href=\"#Final-check-of-shapes\" data-toc-modified-id=\"Final-check-of-shapes-1.6\">Final check of shapes</a></span></li></ul></li><li><span><a href=\"#Defining-the-model,-i.e.-the-NN-architecture\" data-toc-modified-id=\"Defining-the-model,-i.e.-the-NN-architecture-2\">Defining the model, i.e. the NN architecture</a></span></li><li><span><a href=\"#Initializing-the-parameters\" data-toc-modified-id=\"Initializing-the-parameters-3\">Initializing the parameters</a></span></li><li><span><a href=\"#Defining-activation-functions\" data-toc-modified-id=\"Defining-activation-functions-4\">Defining activation functions</a></span></li><li><span><a href=\"#Forward-Propagation\" data-toc-modified-id=\"Forward-Propagation-5\">Forward Propagation</a></span></li><li><span><a href=\"#Computing-the-current-Cost\" data-toc-modified-id=\"Computing-the-current-Cost-6\">Computing the current Cost</a></span></li><li><span><a href=\"#Backpropagation\" data-toc-modified-id=\"Backpropagation-7\">Backpropagation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Network\n",
    "<hr>\n",
    "\n",
    "A DNN can be decomposed as the following sequence of operations:\n",
    "* Input Data\n",
    "* Forward Propagation to obtain an Output Data\n",
    "* Evaluate Output Data (compute current Cost)\n",
    "* Given current Cost, do Back-propagation to update weights\n",
    "* Repeat from beggining with updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run shapesdata.ipynb\n",
    "\n",
    "# Loading the data (circle, square and triangle drawings)\n",
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig, _, _ = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a ['triangle']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a drawing and its label\n",
    "print_img(train_x_orig, train_y_orig, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_orig: (270, 28, 28, 3)\n",
      "train_y_orig: (270, 1)\n",
      "test_x_orig: (30, 28, 28, 3)\n",
      "test_y_orig: (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# Lets see what are the shapes of our variables\n",
    "print(\"train_x_orig: {}\".format(train_x_orig.shape))\n",
    "print(\"train_y_orig: {}\".format(train_y_orig.shape))\n",
    "print(\"test_x_orig: {}\".format(test_x_orig.shape))\n",
    "print(\"test_y_orig: {}\".format(test_y_orig.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets prepare our data\n",
    "\n",
    "### One example by column\n",
    "We can arrange our example either by rows or by columns. Here we choose to arrange them by columns.\n",
    "\n",
    "### Flattening images\n",
    "First, we need to flatten our images, since they are actually arrays and we want them to be vectors. Our **train_x_orig** and **test_x_orig** variables are arrays with shape **(210, 28, 28, 3)**, where the first number stands for the number of examples we have in the set and the remaining three number are a single image array. We want them to be arrays of shape **(28\\*28\\*3, 210)**, so we use numpy's reshape method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data to have feature values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x_flatten/255\n",
    "test_x = test_x_flatten/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (2352, 270)\n",
      "test_x: (2352, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"test_x: {}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding the labels vectors\n",
    "Our **train_y_orig** and **test_y_orig** variables are arrays with shape **(210, 1)**, i.e. column vectors, and to each example there is an associate class indicated by a string, i.e. 'circle'. However, we want this classes to be indicated by numbers and the best way to do that is by a process called **one hot encoding**:\n",
    "* We define a vector whose each component corresponds to a class, and we indicate that our example belongs to a certain class by filling this vector with zeros except for the corresponding class component, which we fill with 1.\n",
    "* To each example, then, we associate one of this vectors.\n",
    "\n",
    "After one hot encoding train_y_orig and test_y_orig we should have labels vectors **train_y** and **test_y** of shape **(3, 210)** and **(3, 90)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencode(y):\n",
    "    \n",
    "    classes = np.unique(y)\n",
    "    \n",
    "    one_hot_y = np.zeros((y.shape[0], len(classes)))\n",
    "    \n",
    "    for i, item in enumerate(y):\n",
    "#         print(i, item)\n",
    "        one_hot_y[i] = item == classes\n",
    "\n",
    "    one_hot_y = one_hot_y.T\n",
    "    \n",
    "    return one_hot_y\n",
    "\n",
    "train_y = onehotencode(train_y_orig)\n",
    "test_y = onehotencode(test_y_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final check of shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (2352, 270)\n",
      "train_y: (3, 270)\n",
      "test_x: (2352, 30)\n",
      "test_y: (3, 30)\n"
     ]
    }
   ],
   "source": [
    "# Lets see what are the new shapes of our variables\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x: {}\".format(test_x.shape))\n",
    "print(\"test_y: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model, i.e. the NN architecture\n",
    "<hr>\n",
    "\n",
    "* Lets denote the number of features of an example by **n_x**\n",
    "* Lets denote the number of classes we can classify to by **C**\n",
    "* Lets denote the number of exampels in the training set by **M**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = train_x.shape[1]                   # Number of examples\n",
    "n_x = train_x.shape[0]                 # Number of features\n",
    "C = 3                                  # Number of classes\n",
    "hidden_layers = [20, 7, 5]             # Number of hidden layers and their respective size\n",
    "\n",
    "layer_dims = [n_x, *hidden_layers, C] # Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "                  including the dimension of the input and the output\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# parameters = initialize_parameters(layer_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \n",
    "    s = 1/(1+np.exp(-x))\n",
    "#     cache = x\n",
    "    \n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    e = 0.01\n",
    "    r = np.maximum(e*x,x)\n",
    "#     cache = x\n",
    "    \n",
    "    return r\n",
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    x_sum = np.sum(x_exp, axis = 0, keepdims = True)\n",
    "    \n",
    "    s = x_exp/x_sum\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "We know that when going from the layer $l-1$ to the layer $l$ we do the following:\n",
    "\\begin{equation}\n",
    "    Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]},\n",
    "\\end{equation}\n",
    "then\n",
    "\\begin{equation}\n",
    "    A^{[l]} = g^{[l]}(Z^{[l]}),\n",
    "\\end{equation}\n",
    "so let's write a code to perform this steps, bearing in mind that we will use them inside the main iteration loop.\n",
    "\n",
    "* Lets denote $A^{[l-1]}$ by **A_prev**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_forward(A_prev, W, b, activation_function = \"relu\"):\n",
    "    \"\"\"\n",
    "    Implement a layer's forward propagation step.\n",
    "\n",
    "    Arguments:\n",
    "        A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "        W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "        b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "        Z -- the input of the activation function, also called pre-activation parameter \n",
    "        cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W @ A_prev + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1])) # This line checks the dimensions of Z\n",
    "    \n",
    "    if activation_function == 'relu':\n",
    "        A = relu(Z)\n",
    "        \n",
    "    if activation_function == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "    if activation_function == 'softmax':\n",
    "        A = softmax(Z)\n",
    "        \n",
    "    assert (A.shape == Z.shape) # This line checks the dimensions of A, which should be the same as of Z\n",
    "\n",
    "        \n",
    "    cache = (A_prev, W, b, Z)\n",
    "    \n",
    "#     return Z, cache\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29969313, 0.29873118],\n",
       "       [0.33790306, 0.34362265],\n",
       "       [0.3624038 , 0.35764616]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function\n",
    "A = np.array([[.1,.2],[.5,.4]])\n",
    "W = np.array([[.2,.3],[.4,.3],[.1, .3]])\n",
    "b = np.array([[.1,.1],[.2,.2],[.3,.3]])\n",
    "\n",
    "test ,_ =step_forward(A, W, b, 'softmax')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    \n",
    "    A = X\n",
    "    L = len(parameters) // 2     # number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\"+str(l)]\n",
    "        b = parameters[\"b\"+str(l)]\n",
    "        \n",
    "        A, cache = step_forward(A_prev, W, b, 'relu')\n",
    "#         print(A.T[l])\n",
    "        caches.append(cache)\n",
    "        \n",
    "    A_prev = A\n",
    "    W = parameters[\"W\"+str(L)]\n",
    "    b = parameters[\"b\"+str(L)]\n",
    "    \n",
    "    AL, cache = step_forward(A_prev, W, b, 'softmax')\n",
    "#     print(AL.T[l])\n",
    "    caches.append(cache)\n",
    "        \n",
    "    assert(AL.shape == (C, X.shape[1]))\n",
    "                \n",
    "    return AL, caches\n",
    "\n",
    "# AL, caches = forward_propagation(train_x, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AL.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the current Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    a = Y*np.log(AL)\n",
    "    \n",
    "    loss = np.sum(a, axis = 0)\n",
    "    cost = (-1)*(1/M)*np.sum(loss)\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost = compute_cost(AL, train_y)\n",
    "# cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_derivative(z):\n",
    "    \n",
    "    x = softmax(z)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return np.multiply(x,(1-x))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    x = sigmoid(z)\n",
    "    return np.multiply(x,(1-x))\n",
    "\n",
    "def relu_derivative(z):\n",
    "    x = np.zeros(z.shape)\n",
    "    x[z > 0] = 1\n",
    "    x[z <= 0] = 0.01\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_step(dA, A_prev, W, b, Z, activation_function = \"relu\"):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    M = A_prev.shape[1]\n",
    "    \n",
    "    if activation_function == 'relu':\n",
    "        dZ = np.multiply(dA, relu_derivative(Z))\n",
    "        \n",
    "    if activation_function == 'sigmoid': \n",
    "        dZ = np.multiply(dA, sigmoid_derivative(Z))\n",
    "        \n",
    "    if activation_function == 'softmax':\n",
    "        dZ = np.multiply(dA,softmax(Z)) - np.diag(dA.T @ softmax(Z))*softmax(Z)\n",
    "        \n",
    "    dW = (dZ @ A_prev.T)\n",
    "    \n",
    "    db = np.sum(dZ, axis = 1, keepdims = True)\n",
    "    \n",
    "    dA_prev = W.T @ dZ\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    M = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "#     dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    dAL = (-Y/AL)/M\n",
    "        \n",
    "    dA = dAL\n",
    "    \n",
    "    A_prev, W, b, Z = caches[L-1]\n",
    "    \n",
    "    dA_prev, dW, db = backward_step(dA, A_prev, W, b, Z, 'softmax')\n",
    "    \n",
    "    grads[\"dA\" + str(L-1)] = dA_prev\n",
    "    grads[\"dW\" + str(L)] = dW\n",
    "    grads[\"db\" + str(L)] = db\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(0, L-1)):   # Note that the first value \"l\" takes is L-2 \n",
    "        \n",
    "        dA = grads[\"dA\" + str(l+1)]  # Note that the first index used is \"l+1\" = L-1, whish follows the L we used\n",
    "        A_prev, W, b, Z = caches[l]\n",
    "    \n",
    "        dA_prev, dW, db = backward_step(dA, A_prev, W, b, Z, 'relu')\n",
    "\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l+1)] = dW\n",
    "        grads[\"db\" + str(l+1)] = db\n",
    "\n",
    "    return grads\n",
    "\n",
    "# grads = backward_propagation(AL, train_y, caches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "    return parameters\n",
    "\n",
    "# parameters = update_parameters(parameters, grads, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(X, Y, dev_x, dev_y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    train_errors = []                         # keep track of train error\n",
    "    test_errors = []                          # keep track of train error\n",
    "    \n",
    "    # Parameters initialization. (â‰ˆ 1 line of code)\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "        dev_AL, _ = forward_propagation(dev_x, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        train_error = compute_cost(AL, Y)\n",
    "        \n",
    "        test_error = compute_cost(dev_AL, dev_y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, train_error))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            train_errors.append(train_error)\n",
    "        if print_cost and i % 100 == 0:\n",
    "            test_errors.append(test_error)\n",
    "        \n",
    "    # plot the cost\n",
    "#     plt.plot(np.squeeze(train_errors))\n",
    "    plt.plot(np.squeeze(test_errors))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    Yhat, _ = forward_propagation(X, parameters)\n",
    "    pred = np.zeros(Yhat.shape).T\n",
    "    for m in range(Yhat.shape[1]):\n",
    "        pred[m][np.argmax(Yhat.T[m], axis = 0)] = 1\n",
    "    return pred.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_right(train_y, train_pred):\n",
    "    prod = train_y*train_pred\n",
    "    \n",
    "#     es = train_pred == train_y\n",
    "    \n",
    "    ans = np.logical_and(*(train_pred == train_y))\n",
    "    ans = ans[np.newaxis, :]\n",
    "    \n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(X, y_true, parameters):\n",
    "    pred = predict(X, parameters)\n",
    "    isright = check_right(y_true, pred)\n",
    "    a = np.unique(isright, return_counts=True)\n",
    "    if len(a[1]) == 2:\n",
    "        percen_wrong = a[1][0]/np.sum(a[1])\n",
    "        percen_right = a[1][1]/np.sum(a[1])\n",
    "    else:\n",
    "        percen_right = 1\n",
    "    return percen_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.466065\n",
      "Cost after iteration 1000: 0.759635\n",
      "Cost after iteration 2000: 0.541124\n",
      "Cost after iteration 3000: 0.486052\n",
      "Cost after iteration 4000: 0.262374\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 5000 iteration, the model achieved a 100% accuracy on the train set.\n",
      "With 5000 iteration, the model achieved a 30.0% accuracy on the test set.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "M = train_x.shape[1]                     # Number of examples\n",
    "n_x = train_x.shape[0]                   # Number of features\n",
    "C = 3                                    # Number of classes\n",
    "hidden_layers = [1000, 50]              # Number of hidden layers and their respective size\n",
    "\n",
    "layer_dims = [n_x, *hidden_layers, C] # Neural Network Architecture\n",
    "\n",
    "num_iter = 5000\n",
    "\n",
    "test_errors = []\n",
    "\n",
    "trained_parameters = deep_model(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    test_x,\n",
    "    test_y,\n",
    "    layer_dims,\n",
    "    learning_rate = 0.003, \n",
    "    num_iterations = num_iter, \n",
    "    print_cost=True)\n",
    "\n",
    "acc_train = get_score(train_x, train_y, trained_parameters)\n",
    "acc_test = get_score(test_x, test_y, trained_parameters)\n",
    "\n",
    "print(\"With {} iteration, the model achieved a {}% accuracy on the train set.\".format(num_iter, acc_train*100))\n",
    "print(\"With {} iteration, the model achieved a {}% accuracy on the test set.\".format(num_iter, acc_test*100))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
