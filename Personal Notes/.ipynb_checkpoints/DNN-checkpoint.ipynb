{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Network\n",
    "<hr>\n",
    "\n",
    "A DNN can be decomposed as the following sequence of operations:\n",
    "* Input Data\n",
    "* Forward Propagation to obtain an Output Data\n",
    "* Evaluate Output Data (compute current Cost)\n",
    "* Given current Cost, do Back-propagation to update weights\n",
    "* Repeat from beggining with updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run shapesdata.ipynb\n",
    "\n",
    "# Loading the data (circle, square and triangle drawings)\n",
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig, _, _ = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a ['triangle']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a drawing and its label\n",
    "print_img(train_x_orig, train_y_orig, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_orig: (210, 28, 28, 3)\n",
      "train_y_orig: (210, 1)\n",
      "test_x_orig: (90, 28, 28, 3)\n",
      "test_y_orig: (90, 1)\n"
     ]
    }
   ],
   "source": [
    "# Lets see what are the shapes of our variables\n",
    "print(\"train_x_orig: {}\".format(train_x_orig.shape))\n",
    "print(\"train_y_orig: {}\".format(train_y_orig.shape))\n",
    "print(\"test_x_orig: {}\".format(test_x_orig.shape))\n",
    "print(\"test_y_orig: {}\".format(test_y_orig.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets prepare our data\n",
    "\n",
    "### One example by column\n",
    "We can arrange our example either by rows or by columns. Here we choose to arrange them by columns.\n",
    "\n",
    "### Flattening images\n",
    "First, we need to flatten our images, since they are actually arrays and we want them to be vectors. Our **train_x_orig** and **test_x_orig** variables are arrays with shape **(210, 28, 28, 3)**, where the first number stands for the number of examples we have in the set and the remaining three number are a single image array. We want them to be arrays of shape **(28\\*28\\*3, 210)**, so we use numpy's reshape method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data to have feature values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x_flatten/255\n",
    "test_x = test_x_flatten/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (2352, 210)\n",
      "test_x: (2352, 90)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"test_x: {}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding the labels vectors\n",
    "Our **train_y_orig** and **test_y_orig** variables are arrays with shape **(210, 1)**, i.e. column vectors, and to each example there is an associate class indicated by a string, i.e. 'circle'. However, we want this classes to be indicated by numbers and the best way to do that is by a process called **one hot encoding**:\n",
    "* We define a vector whose each component corresponds to a class, and we indicate that our example belongs to a certain class by filling this vector with zeros except for the corresponding class component, which we fill with 1.\n",
    "* To each example, then, we associate one of this vectors.\n",
    "\n",
    "After one hot encoding train_y_orig and test_y_orig we should have labels vectors **train_y** and **test_y** of shape **(3, 210)** and **(3, 90)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencode(y):\n",
    "    \n",
    "    classes = np.unique(y)\n",
    "    \n",
    "    one_hot_y = np.zeros((len(y[0]), len(classes)))\n",
    "    \n",
    "    for i, item in enumerate(y[0]):   \n",
    "        one_hot_y[i] = item == classes\n",
    "\n",
    "    one_hot_y = one_hot_y.T\n",
    "    \n",
    "    return one_hot_y\n",
    "\n",
    "train_y = onehotencode(train_y_orig)\n",
    "test_y = onehotencode(test_y_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final check of shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (2352, 210)\n",
      "train_y: (3, 210)\n",
      "test_x: (2352, 90)\n",
      "test_y: (3, 90)\n"
     ]
    }
   ],
   "source": [
    "# Lets see what are the new shapes of our variables\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x: {}\".format(test_x.shape))\n",
    "print(\"test_y: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model, i.e. the NN architecture\n",
    "<hr>\n",
    "\n",
    "* Lets denote the number of features of an example by **n_x**\n",
    "* Lets denote the number of classes we can classify to by **C**\n",
    "* Lets denote the number of exampels in the training set by **M**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = train_x.shape[1]                   # Number of examples\n",
    "n_x = train_x.shape[0]                 # Number of features\n",
    "C = 3                                  # Number of classes\n",
    "hidden_layers = [20, 7, 5]             # Number of hidden layers and their respective size\n",
    "\n",
    "layer_dims = [n_x, *hidden_layers, C] # Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "                  including the dimension of the input\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters(architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \n",
    "    s = 1/(1+np.exp(-x))\n",
    "#     cache = x\n",
    "    \n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    e = 0.01\n",
    "    r = np.maximum(e*x,x)\n",
    "#     cache = x\n",
    "    \n",
    "    return r\n",
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    x_sum = np.sum(x_exp, axis = 0, keepdims = True)\n",
    "    \n",
    "    s = x_exp/x_sum\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "We know that when going from the layer $l-1$ to the layer $l$ we do the following:\n",
    "\\begin{equation}\n",
    "    Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]},\n",
    "\\end{equation}\n",
    "then\n",
    "\\begin{equation}\n",
    "    A^{[l]} = g^{[l]}(Z^{[l]}),\n",
    "\\end{equation}\n",
    "so let's write a code to perform this steps, bearing in mind that we will use them inside the main iteration loop.\n",
    "\n",
    "* Lets denote $A^{[l-1]}$ by **A_prev**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_forward(A_prev, W, b, activation_function = \"relu\"):\n",
    "    \"\"\"\n",
    "    Implement a layer's forward propagation step.\n",
    "\n",
    "    Arguments:\n",
    "        A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "        W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "        b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "        Z -- the input of the activation function, also called pre-activation parameter \n",
    "        cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W @ A_prev + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1])) # This line checks the dimensions of Z\n",
    "    \n",
    "    if activation_function == 'relu':\n",
    "        A = relu(Z)\n",
    "        \n",
    "    if activation_function == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "    if activation_function == 'softmax':\n",
    "        A = softmax(Z)\n",
    "        \n",
    "    assert (A.shape == Z.shape) # This line checks the dimensions of A, which should be the same as of Z\n",
    "\n",
    "        \n",
    "    cache = (A_prev, W, b, Z)\n",
    "    \n",
    "#     return Z, cache\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29969313, 0.29873118],\n",
       "       [0.33790306, 0.34362265],\n",
       "       [0.3624038 , 0.35764616]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function\n",
    "A = np.array([[.1,.2],[.5,.4]])\n",
    "W = np.array([[.2,.3],[.4,.3],[.1, .3]])\n",
    "b = np.array([[.1,.1],[.2,.2],[.3,.3]])\n",
    "\n",
    "step_forward(A, W, b, 'softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    \n",
    "    A = X\n",
    "    L = len(parameters) // 2     # number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\"+str(l)]\n",
    "        b = parameters[\"b\"+str(l)]\n",
    "        \n",
    "        A, cache = step_forward(A_prev, W, b, 'relu')\n",
    "#         print(A.T[l])\n",
    "        caches.append(cache)\n",
    "        \n",
    "    A_prev = A\n",
    "    W = parameters[\"W\"+str(L)]\n",
    "    b = parameters[\"b\"+str(L)]\n",
    "    \n",
    "    AL, cache = step_forward(A_prev, W, b, 'softmax')\n",
    "#     print(AL.T[l])\n",
    "    caches.append(cache)\n",
    "        \n",
    "    assert(AL.shape == (C, X.shape[1]))\n",
    "                \n",
    "    return AL, caches\n",
    "\n",
    "AL, caches = forward_propagation(train_x, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12585737, 0.21361025, 0.66053238])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the current Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    x = np.log(AL)\n",
    "    a = Y*np.log(AL)\n",
    "#     print(a.shape)\n",
    "    loss = np.sum(a, axis = 0)\n",
    "    cost = (-1)*(1/M)*np.sum(loss)\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0987873841165654"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = compute_cost(AL, train_y)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_derivative(z):\n",
    "    x = softmax(z)\n",
    "    return np.multiply(x,(1-x))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    x = sigmoid(z)\n",
    "    return np.multiply(x,(1-x))\n",
    "\n",
    "def relu_derivative(z):\n",
    "    x = np.zeros(z.shape)\n",
    "    x[z > 0] = 1\n",
    "    x[z <= 0] = 0.01\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_step(dA, A_prev, W, b, Z, activation_function = \"relu\"):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation_function == 'relu': \n",
    "        dZ = np.multiply(dA, relu_derivative(Z))\n",
    "        \n",
    "    if activation_function == 'sigmoid': \n",
    "        dZ = np.multiply(dA, sigmoid_derivative(Z))\n",
    "        \n",
    "    if activation_function == 'softmax': \n",
    "        dZ = np.multiply(dA, softmax_derivative(Z))\n",
    "    \n",
    "    dW = (1/M)*(dZ @ A_prev.T)\n",
    "    \n",
    "    db = (1/M)*np.sum(dZ, axis = 1, keepdims = True)\n",
    "    \n",
    "    dA_prev = W.T @ dZ\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "#     dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    dAL = AL - Y\n",
    "        \n",
    "    dA = dAL\n",
    "    \n",
    "    A_prev, W, b, Z = caches[L-1]\n",
    "    \n",
    "    dA_prev, dW, db = backward_step(dA, A_prev, W, b, Z, 'softmax')\n",
    "    \n",
    "    grads[\"dA\" + str(L-1)] = dA_prev\n",
    "    grads[\"dW\" + str(L)] = dW\n",
    "    grads[\"db\" + str(L)] = db\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(0, L-1)):   # Note that the first value \"l\" takes is L-2 \n",
    "        \n",
    "        dA = grads[\"dA\" + str(l+1)]  # Note that the first index used is \"l+1\" = L-1, whish follows the L we used\n",
    "        A_prev, W, b, Z = caches[l]\n",
    "    \n",
    "        dA_prev, dW, db = backward_step(dA, A_prev, W, b, Z, 'relu')\n",
    "\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l+1)] = dW\n",
    "        grads[\"db\" + str(l+1)] = db\n",
    "\n",
    "    return grads\n",
    "\n",
    "grads = model_backward(AL, train_y, caches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "    return parameters\n",
    "\n",
    "parameters = update_parameters(parameters, grads, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(X, Y, dev_x, dev_y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    train_errors = []                         # keep track of train error\n",
    "    test_errors = []                          # keep track of train error\n",
    "    \n",
    "    # Parameters initialization. (â‰ˆ 1 line of code)\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "        dev_AL, _ = forward_propagation(dev_x, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        train_error = compute_cost(AL, Y)\n",
    "        \n",
    "        test_error = compute_cost(dev_AL, dev_y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, train_error))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            train_errors.append(train_error)\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            test_errors.append(test_error)\n",
    "        \n",
    "    # plot the cost\n",
    "#     plt.plot(np.squeeze(train_errors))\n",
    "#     plt.plot(np.squeeze(test_errors))\n",
    "#     plt.ylabel('cost')\n",
    "#     plt.xlabel('iterations (per tens)')\n",
    "#     plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "#     plt.show()\n",
    "    \n",
    "    return parameters, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    Yhat, _ = forward_propagation(X, parameters)\n",
    "    pred = np.zeros(Yhat.shape).T\n",
    "    for m in range(Yhat.shape[1]):\n",
    "        pred[m][np.argmax(Yhat.T[m], axis = 0)] = 1\n",
    "    return pred.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_error(train_y, train_pred):\n",
    "    prod = train_y*train_pred\n",
    "    count = np.sum(prod)\n",
    "    \n",
    "    percen_acc = count/train_y.shape[1]\n",
    "    percen_error = 1-percen_acc\n",
    "    \n",
    "    percen_acc = np.around(percen_acc,3)\n",
    "    percen_error = np.around(percen_error,3)\n",
    "    \n",
    "    return (percen_acc, percen_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1000 iteration, the model achieved a 66.7% error on the train set.\n",
      "With 1000 iteration, the model achieved a 66.7% error on the test set.\n",
      "With 2000 iteration, the model achieved a 67.10000000000001% error on the train set.\n",
      "With 2000 iteration, the model achieved a 64.4% error on the test set.\n",
      "With 5000 iteration, the model achieved a 64.3% error on the train set.\n",
      "With 5000 iteration, the model achieved a 63.3% error on the test set.\n",
      "With 10000 iteration, the model achieved a 64.8% error on the train set.\n",
      "With 10000 iteration, the model achieved a 62.2% error on the test set.\n",
      "With 25000 iteration, the model achieved a 64.3% error on the train set.\n",
      "With 25000 iteration, the model achieved a 64.4% error on the test set.\n",
      "With 100000 iteration, the model achieved a 62.9% error on the train set.\n",
      "With 100000 iteration, the model achieved a 62.2% error on the test set.\n"
     ]
    }
   ],
   "source": [
    "num_iter = [1000, 2000, 5000, 10000]\n",
    "\n",
    "for num in num_iter:\n",
    "    trained_parameters, grads = deep_model(\n",
    "        train_x, \n",
    "        train_y, \n",
    "        test_x,\n",
    "        test_y,\n",
    "        layers_dims,\n",
    "        learning_rate = 0.01, \n",
    "        num_iterations = num, \n",
    "        print_cost=False)\n",
    "\n",
    "    train_pred = predict(train_x, trained_parameters)\n",
    "    test_pred = predict(test_x, trained_parameters)\n",
    "    \n",
    "    train_acc, train_err = acc_error(train_y, train_pred)\n",
    "    test_acc, test_err = acc_error(test_y, test_pred)\n",
    "    print(\"With {} iteration, the model achieved a {}% error on the train set.\".format(num, train_err*100))\n",
    "    print(\"With {} iteration, the model achieved a {}% error on the test set.\".format(num, test_err*100))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 1000 iteration, the model achieved a 31.4% error on the train set.\n",
      "With 1000 iteration, the model achieved a 54.400000000000006% error on the test set.\n",
      "\n",
      "\n",
      "With 2000 iteration, the model achieved a 35.199999999999996% error on the train set.\n",
      "With 2000 iteration, the model achieved a 55.60000000000001% error on the test set.\n",
      "\n",
      "\n",
      "With 5000 iteration, the model achieved a 7.1% error on the train set.\n",
      "With 5000 iteration, the model achieved a 55.60000000000001% error on the test set.\n",
      "\n",
      "\n",
      "With 10000 iteration, the model achieved a 3.8% error on the train set.\n",
      "With 10000 iteration, the model achieved a 58.9% error on the test set.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_iter = [1000, 5000, 10000]\n",
    "\n",
    "for num in num_iter:\n",
    "    trained_parameters, grads = deep_model(\n",
    "        train_x, \n",
    "        train_y, \n",
    "        test_x,\n",
    "        test_y,\n",
    "        layers_dims,\n",
    "        learning_rate = 0.01, \n",
    "        num_iterations = num, \n",
    "        print_cost=False)\n",
    "\n",
    "    train_pred = predict(train_x, trained_parameters)\n",
    "    test_pred = predict(test_x, trained_parameters)\n",
    "    \n",
    "    train_acc, train_err = acc_error(train_y, train_pred)\n",
    "    test_acc, test_err = acc_error(test_y, test_pred)\n",
    "    print(\"With {} iteration, the model achieved a {}% error on the train set.\".format(num, train_err*100))\n",
    "    print(\"With {} iteration, the model achieved a {}% error on the test set.\".format(num, test_err*100))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
