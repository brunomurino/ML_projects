{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Neural Network\n",
    "<hr>\n",
    "\n",
    "A DNN can be decomposed as the following sequence of operations:\n",
    "* Input Data\n",
    "* Forward Propagation to obtain an Output Data\n",
    "* Evaluate Output Data (compute current Cost)\n",
    "* Given current Cost, do Back-propagation to update weights\n",
    "* Repeat from beggining with updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run shapesdata.ipynb\n",
    "\n",
    "# Loading the data (circle, square and triangle drawings)\n",
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig, _, _ = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a ['triangle']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a drawing and its label\n",
    "print_img(train_x_orig, train_y_orig, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_orig: (210, 28, 28, 3)\n",
      "train_y_orig: (210, 1)\n",
      "test_x_orig: (90, 28, 28, 3)\n",
      "test_y_orig: (90, 1)\n"
     ]
    }
   ],
   "source": [
    "# Lets see what are the shapes of our variables\n",
    "print(\"train_x_orig: {}\".format(train_x_orig.shape))\n",
    "print(\"train_y_orig: {}\".format(train_y_orig.shape))\n",
    "print(\"test_x_orig: {}\".format(test_x_orig.shape))\n",
    "print(\"test_y_orig: {}\".format(test_y_orig.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets prepare our data\n",
    "\n",
    "### One example by column\n",
    "We can arrange our example either by rows or by columns. Here we choose to arrange them by columns.\n",
    "\n",
    "### Flattening images\n",
    "First, we need to flatten our images, since they are actually arrays and we want them to be vectors. Our **train_x_orig** and **test_x_orig** variables are arrays with shape **(210, 28, 28, 3)**, where the first number stands for the number of examples we have in the set and the remaining three number are a single image array. We want them to be arrays of shape **(28\\*28\\*3, 210)**, so we use numpy's reshape method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data to have feature values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x_flatten/255\n",
    "test_x = test_x_flatten/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (2352, 210)\n",
      "test_x: (2352, 90)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"test_x: {}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding the labels vectors\n",
    "Our **train_y_orig** and **test_y_orig** variables are arrays with shape **(210, 1)**, i.e. column vectors, and to each example there is an associate class indicated by a string, i.e. 'circle'. However, we want this classes to be indicated by numbers and the best way to do that is by a process called **one hot encoding**:\n",
    "* We define a vector whose each component corresponds to a class, and we indicate that our example belongs to a certain class by filling this vector with zeros except for the corresponding class component, which we fill with 1.\n",
    "* To each example, then, we associate one of this vectors.\n",
    "\n",
    "After one hot encoding train_y_orig and test_y_orig we should have labels vectors **train_y** and **test_y** of shape **(3, 210)** and **(3, 90)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencode(y):\n",
    "    \n",
    "    classes = np.unique(y)\n",
    "    \n",
    "    one_hot_y = np.zeros((len(y[0]), len(classes)))\n",
    "    \n",
    "    for i, item in enumerate(y[0]):   \n",
    "        one_hot_y[i] = item == classes\n",
    "\n",
    "    one_hot_y = one_hot_y.T\n",
    "    \n",
    "    return one_hot_y\n",
    "\n",
    "train_y = onehotencode(train_y_orig)\n",
    "test_y = onehotencode(test_y_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final check of shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x: (2352, 210)\n",
      "train_y: (3, 210)\n",
      "test_x: (2352, 90)\n",
      "test_y: (3, 90)\n"
     ]
    }
   ],
   "source": [
    "# Lets see what are the new shapes of our variables\n",
    "print(\"train_x: {}\".format(train_x.shape))\n",
    "print(\"train_y: {}\".format(train_y.shape))\n",
    "print(\"test_x: {}\".format(test_x.shape))\n",
    "print(\"test_y: {}\".format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model, i.e. the NN architecture\n",
    "<hr>\n",
    "\n",
    "* Lets denote the number of features of an example by **n_x**\n",
    "* Lets denote the number of classes we can classify to by **C**\n",
    "* Lets denote the number of exampels in the training set by **M**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = train_x.shape[1]                   # Number of examples\n",
    "n_x = train_x.shape[0]                 # Number of features\n",
    "C = 3                                  # Number of classes\n",
    "hidden_layers = [20, 7, 5]             # Number of hidden layers and their respective size\n",
    "\n",
    "layer_dims = [n_x, *hidden_layers, C] # Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "                  including the dimension of the input and the output\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters(architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \n",
    "    s = 1/(1+np.exp(-x))\n",
    "#     cache = x\n",
    "    \n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    e = 0.01\n",
    "    r = np.maximum(e*x,x)\n",
    "#     cache = x\n",
    "    \n",
    "    return r\n",
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    x_exp = np.exp(x)\n",
    "\n",
    "    x_sum = np.sum(x_exp, axis = 0, keepdims = True)\n",
    "    \n",
    "    s = x_exp/x_sum\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "We know that when going from the layer $l-1$ to the layer $l$ we do the following:\n",
    "\\begin{equation}\n",
    "    Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]},\n",
    "\\end{equation}\n",
    "then\n",
    "\\begin{equation}\n",
    "    A^{[l]} = g^{[l]}(Z^{[l]}),\n",
    "\\end{equation}\n",
    "so let's write a code to perform this steps, bearing in mind that we will use them inside the main iteration loop.\n",
    "\n",
    "* Lets denote $A^{[l-1]}$ by **A_prev**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_forward(A_prev, W, b, activation_function = \"relu\"):\n",
    "    \"\"\"\n",
    "    Implement a layer's forward propagation step.\n",
    "\n",
    "    Arguments:\n",
    "        A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "        W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "        b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "        Z -- the input of the activation function, also called pre-activation parameter \n",
    "        cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = W @ A_prev + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A_prev.shape[1])) # This line checks the dimensions of Z\n",
    "    \n",
    "    if activation_function == 'relu':\n",
    "        A = relu(Z)\n",
    "        \n",
    "    if activation_function == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "    if activation_function == 'softmax':\n",
    "        A = softmax(Z)\n",
    "        \n",
    "    assert (A.shape == Z.shape) # This line checks the dimensions of A, which should be the same as of Z\n",
    "\n",
    "        \n",
    "    cache = (A_prev, W, b, Z)\n",
    "    \n",
    "#     return Z, cache\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29969313, 0.29873118],\n",
       "       [0.33790306, 0.34362265],\n",
       "       [0.3624038 , 0.35764616]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the function\n",
    "A = np.array([[.1,.2],[.5,.4]])\n",
    "W = np.array([[.2,.3],[.4,.3],[.1, .3]])\n",
    "b = np.array([[.1,.1],[.2,.2],[.3,.3]])\n",
    "\n",
    "step_forward(A, W, b, 'softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    \n",
    "    A = X\n",
    "    L = len(parameters) // 2     # number of layers in the neural network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\"+str(l)]\n",
    "        b = parameters[\"b\"+str(l)]\n",
    "        \n",
    "        A, cache = step_forward(A_prev, W, b, 'relu')\n",
    "#         print(A.T[l])\n",
    "        caches.append(cache)\n",
    "        \n",
    "    A_prev = A\n",
    "    W = parameters[\"W\"+str(L)]\n",
    "    b = parameters[\"b\"+str(L)]\n",
    "    \n",
    "    AL, cache = step_forward(A_prev, W, b, 'softmax')\n",
    "#     print(AL.T[l])\n",
    "    caches.append(cache)\n",
    "        \n",
    "    assert(AL.shape == (C, X.shape[1]))\n",
    "                \n",
    "    return AL, caches\n",
    "\n",
    "AL, caches = forward_propagation(train_x, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12585737, 0.21361025, 0.66053238])"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL.T[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the current Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    x = np.log(AL)\n",
    "    a = Y*np.log(AL)\n",
    "#     print(a.shape)\n",
    "    loss = np.sum(a, axis = 0)\n",
    "    cost = (-1)*(1/M)*np.sum(loss)\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0987873841165654"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = compute_cost(AL, train_y)\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_derivative(z):\n",
    "    x = softmax(z)\n",
    "    return np.multiply(x,(1-x))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    x = sigmoid(z)\n",
    "    return np.multiply(x,(1-x))\n",
    "\n",
    "def relu_derivative(z):\n",
    "    x = np.zeros(z.shape)\n",
    "    x[z > 0] = 1\n",
    "    x[z <= 0] = 0.01\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_step(dA, A_prev, W, b, Z, activation_function = \"relu\"):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation_function == 'relu': \n",
    "        dZ = np.multiply(dA, relu_derivative(Z))\n",
    "        \n",
    "    if activation_function == 'sigmoid': \n",
    "        dZ = np.multiply(dA, sigmoid_derivative(Z))\n",
    "        \n",
    "    if activation_function == 'softmax': \n",
    "        dZ = np.multiply(dA, softmax_derivative(Z))\n",
    "    \n",
    "    dW = (1/M)*(dZ @ A_prev.T)\n",
    "    \n",
    "    db = (1/M)*np.sum(dZ, axis = 1, keepdims = True)\n",
    "    \n",
    "    dA_prev = W.T @ dZ\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "#     dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    dAL = AL - Y\n",
    "        \n",
    "    dA = dAL\n",
    "    \n",
    "    A_prev, W, b, Z = caches[L-1]\n",
    "    \n",
    "    dA_prev, dW, db = backward_step(dA, A_prev, W, b, Z, 'softmax')\n",
    "    \n",
    "    grads[\"dA\" + str(L-1)] = dA_prev\n",
    "    grads[\"dW\" + str(L)] = dW\n",
    "    grads[\"db\" + str(L)] = db\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(0, L-1)):   # Note that the first value \"l\" takes is L-2 \n",
    "        \n",
    "        dA = grads[\"dA\" + str(l+1)]  # Note that the first index used is \"l+1\" = L-1, whish follows the L we used\n",
    "        A_prev, W, b, Z = caches[l]\n",
    "    \n",
    "        dA_prev, dW, db = backward_step(dA, A_prev, W, b, Z, 'relu')\n",
    "\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l+1)] = dW\n",
    "        grads[\"db\" + str(l+1)] = db\n",
    "\n",
    "    return grads\n",
    "\n",
    "grads = model_backward(AL, train_y, caches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "    return parameters\n",
    "\n",
    "parameters = update_parameters(parameters, grads, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_model(X, Y, dev_x, dev_y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(42)\n",
    "    train_errors = []                         # keep track of train error\n",
    "    test_errors = []                          # keep track of train error\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "        \n",
    "        dev_AL, _ = forward_propagation(dev_x, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        train_error = compute_cost(AL, Y)\n",
    "        \n",
    "        test_error = compute_cost(dev_AL, dev_y)\n",
    "    \n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, train_error))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            train_errors.append(train_error)\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            test_errors.append(test_error)\n",
    "        \n",
    "    # plot the cost\n",
    "#     plt.plot(np.squeeze(train_errors))\n",
    "#     plt.plot(np.squeeze(test_errors))\n",
    "#     plt.ylabel('cost')\n",
    "#     plt.xlabel('iterations (per tens)')\n",
    "#     plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "#     plt.show()\n",
    "    \n",
    "    return parameters, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    Yhat, _ = forward_propagation(X, parameters)\n",
    "    pred = np.zeros(Yhat.shape).T\n",
    "    for m in range(Yhat.shape[1]):\n",
    "        pred[m][np.argmax(Yhat.T[m], axis = 0)] = 1\n",
    "    return pred.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_error(train_y, train_pred):\n",
    "    prod = train_y*train_pred\n",
    "    verify = np.sum(prod, axis = 0)\n",
    "    count = np.sum(verify)\n",
    "    \n",
    "    percen_acc = count/train_y.shape[1]\n",
    "    percen_error = 1-percen_acc\n",
    "    \n",
    "    percen_acc = np.around(percen_acc,3)\n",
    "    percen_error = np.around(percen_error,3)\n",
    "      \n",
    "#     print(train_pred.T[verify == 0])\n",
    "#     print(train_y.T[verify == 0])\n",
    "    \n",
    "    train_mis = train_pred.T[verify == 0]\n",
    "    \n",
    "    return (percen_acc, percen_error)\n",
    "\n",
    "# train_acc, train_err = acc_error(train_y, train_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = np.linspace(1000,5000,21)\n",
    "\n",
    "test_errors = []\n",
    "\n",
    "for num in num_iter:\n",
    "    trained_parameters, grads = deep_model(\n",
    "        train_x, \n",
    "        train_y, \n",
    "        test_x,\n",
    "        test_y,\n",
    "        layers_dims,\n",
    "        learning_rate = 0.01, \n",
    "        num_iterations = int(num), \n",
    "        print_cost=False)\n",
    "\n",
    "    train_pred = predict(train_x, trained_parameters)\n",
    "    test_pred = predict(test_x, trained_parameters)\n",
    "\n",
    "    train_acc, train_err = acc_error(train_y, train_pred)\n",
    "    test_acc, test_err = acc_error(test_y, test_pred)\n",
    "    \n",
    "    test_errors.append(test_err)\n",
    "#     print(\"With {} iteration, the model achieved a {}% error on the train set.\".format(num, train_err*100))\n",
    "#     print(\"With {} iteration, the model achieved a {}% error on the test set.\".format(num, test_err*100))\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(num_iter,test_errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
