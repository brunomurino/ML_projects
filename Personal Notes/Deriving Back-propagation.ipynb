{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\pdv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\dd}[1]{\\textit{d}#1}\n",
    "\\newcommand{\\softmax}[1]{\\Softmax\\left(#1\\right)}\n",
    "\\newcommand{\\exp}[1]{e^{#1}}\n",
    "\\newcommand{\\grad}{\\nabla}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\N}{\\mathbb{N}}\n",
    "\\newcommand{\\set}[1]{\\left\\{#1\\right\\}}\n",
    "\\newcommand{\\idm}{\\mathbb{1}}  % \\idm identity matrix\n",
    "\\DeclareMathOperator{\\Softmax}{softmax}\n",
    "\\DeclareMathOperator{\\mat}{Mat}\n",
    "\\DeclareMathOperator{\\GL}{GL}\n",
    "\\DeclareMathOperator{\\SL}{SL}\n",
    "\\DeclareMathOperator{\\diag}{diag}\n",
    "\\DeclareMathOperator{\\sgn}{sgn}\n",
    "\\DeclareMathOperator{\\lexp}{exp}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Defining-the-model-and-its-variables\" data-toc-modified-id=\"Defining-the-model-and-its-variables-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Defining the model and its variables</a></span></li><li><span><a href=\"#Finding-$\\grad_{Z^{[L](m)}}-J$\" data-toc-modified-id=\"Finding-$\\grad_{Z^{[L](m)}}-J$-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Finding $\\grad_{Z^{[L](m)}} J$</a></span><ul class=\"toc-item\"><li><span><a href=\"#Finding-$\\grad_{A^{[L](m)}}-L^{(m)}$\" data-toc-modified-id=\"Finding-$\\grad_{A^{[L](m)}}-L^{(m)}$-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Finding $\\grad_{A^{[L](m)}} L^{(m)}$</a></span></li><li><span><a href=\"#Finding-$G^{[L](m)}$\" data-toc-modified-id=\"Finding-$G^{[L](m)}$-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Finding $G^{[L](m)}$</a></span></li><li><span><a href=\"#Back-to-$\\grad_{Z^{[L](m)}}-J$\" data-toc-modified-id=\"Back-to-$\\grad_{Z^{[L](m)}}-J$-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Back to $\\grad_{Z^{[L](m)}} J$</a></span></li></ul></li><li><span><a href=\"#Finding-$\\pdv{J}{W^{[L]}}$-and-$\\pdv{J}{b^{[L]}}$\" data-toc-modified-id=\"Finding-$\\pdv{J}{W^{[L]}}$-and-$\\pdv{J}{b^{[L]}}$-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Finding $\\pdv{J}{W^{[L]}}$ and $\\pdv{J}{b^{[L]}}$</a></span></li><li><span><a href=\"#Finding-$\\pdv{J}{W^{[L-1]}}$-and-$\\pdv{J}{b^{[L-1]}}$\" data-toc-modified-id=\"Finding-$\\pdv{J}{W^{[L-1]}}$-and-$\\pdv{J}{b^{[L-1]}}$-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Finding $\\pdv{J}{W^{[L-1]}}$ and $\\pdv{J}{b^{[L-1]}}$</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving Back-propagation for Multi-classification NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model and its variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable | Denoted by |Belongs to / Domain\n",
    "--|------|------\n",
    "Number of examples on training set| $M$ | $\\N$ \n",
    "Number of layers (excluding input layer) | $L$|$\\N$ \n",
    "Layer index | $[\\ell]$ | $\\ell = 0,1,2,\\cdots,L$\n",
    "Example index | $(m)$ | $m = 1,2,\\cdots,M$\n",
    "Number of input features per example in the $(\\ell)$-th layer |$n^{[\\ell]}$| $\\N$\n",
    "Number of input features per example in the $(0)$-th layer |$n_x = n^{[0]}$| $\\N$\n",
    "Input matrix of the $(\\ell)$-th layer|   $A^{[\\ell]}$  | $\\mat(n^{[\\ell]}, M)$\n",
    "Starting input matrix ($0$-th layer) | $X = A^{[0]}$  | $\\mat(n^{[0]}, M)$\n",
    "Unactivated input matrix of the $(\\ell)$-th layer|   $Z^{[\\ell]}$  | $\\mat(n^{[\\ell]}, M)$\n",
    "Activation function of the $(\\ell)$-th layer (per example) | $g^{[\\ell](m)}$| $g^{[\\ell](m)}: \\mat(n^{[\\ell]}, 1) \\to \\mat(n^{[\\ell]}, 1)$\n",
    "Weights matrix to go from the $(\\ell-1)$-th layer to the $(\\ell)$-th layer | $W^{[\\ell]}$| $\\mat(n^{[\\ell]}, n^{[\\ell-1]})$\n",
    "Bias matrix to go from the $(\\ell-1)$-th layer to the $(\\ell)$-th layer | $b^{[\\ell]}$| $\\mat(n^{[\\ell]}, M)$\n",
    "Number of output features | $C = n^{[L]}$ | $\\N$ \n",
    "Model output | $Y = A^{[L]}$ | $\\mat(n^{[L]}, M)$\n",
    "$m$-th example of the model output | $Y^{(m)} = A^{[L](m)}$ | $\\mat(n^{[L]}, 1)$\n",
    "Desired output |  $D $  | $\\mat(n^{[L]}, M)$\n",
    "$m$-th example of the desired output | $D^{(m)}$ | $\\mat(n^{[L]}, 1)$\n",
    "$m$-th example of the input matrix of the $(\\ell)$-th layer| $A^{[\\ell](m)}$ | $\\mat(n^{[\\ell]}, 1)$\n",
    "$m$-th example of the unactivated input matrix of the $(\\ell)$-th layer| $Z^{[\\ell](m)}$ | $\\mat(n^{[\\ell]}, 1)$\n",
    "Loss function (single example error) | $L^{(m)} = L(D^{(m)}, Y^{(m)})$ | $L:\\mat(n^{[L]}, 1)\\times \\mat(n^{[L]}, 1)  \\to \\R$\n",
    "Error function |  $J(L^{(1)},\\cdots,L^{(M)})$ |  $J:\\R^{M} \\to \\R$\n",
    "Vector of lenght $a$ with elements all $1$| $1_{a}$ | $\\mat(a,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's state how to go from the $(\\ell-1)$-th layer  to the $\\ell$-th layer:\n",
    "\\begin{equation}\n",
    "    A^{[\\ell]} = g^{[\\ell]}\\Big(Z^{[\\ell]}\\Big) = g^{[\\ell]}\\Big(W^{[\\ell]}A^{[\\ell-1]} + b^{[\\ell]} \\Big).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's state our Error function:\n",
    "\\begin{equation}\n",
    "    J(L^{(1)},\\cdots,L^{(M)}) = \\frac{1}{M}\\sum_{m=1}^{M} L(D^{(m)}, Y^{(m)}).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to derive back-propagation for a multi-classification NN, let's set the loss function as the cross-entropy:\n",
    "\\begin{equation}\\label{eq:crossentropydef}\n",
    "    L^{(m)} = - \\sum_{q=1}^{n^{[L]}} D_q^{(m)} \\ln(Y_q^{(m)})= - \\sum_{q=1}^{n^{[L]}} D_q^{(m)} \\ln(A_q^{[L](m)}),\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And at last, let's set our last activation function, $g^{[L]}$, as the softmax function:\n",
    "\\begin{equation}\n",
    "    g^{[L](m)} = \\softmax{Z^{[L](m)}},\n",
    "\\end{equation}\n",
    "with\n",
    "\\begin{equation}\\label{eq:softmaxdef}\n",
    "    \\softmax{Z^{[L](m)}}_k = \\frac{\\exp{Z_k^{[L](m)}}}{\\sum_{r=1}^{n^{[L]}} \\exp{Z_r^{[L](m)}}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding $\\grad_{Z^{[L](m)}} J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start computing $\\pdv{J}{Z^{[L](m)}}$. By the chain rule:\n",
    "\\begin{equation}\n",
    "    \\grad_{Z^{[L](m)}} J = \\pdv{J}{Z^{[L](m)}} = \\frac{1}{M}\\sum_{m'=1}^{M} \\pdv{L^{(m')}}{Z^{[L](m)}} = \\frac{1}{M} \\pdv{L^{(m)}}{Z^{[L](m)}} = \\frac{1}{M}\\grad_{Z^{[L](m)}} L^{(m)},\n",
    "\\end{equation}\n",
    "and then again:\n",
    "\\begin{equation}\n",
    "    \\grad_{Z^{[L](m)}} L^{(m)} = \\grad_{A^{[L](m)}} L^{(m)}\\pdv{A^{[L](m)}}{Z^{[L](m)}}.\n",
    "\\end{equation}\n",
    "\n",
    "Since both $A^{[L](m)}$ and $Z^{[L](m)}$ are vectors, the term $\\pdv{A^{[L](m)}}{Z^{[L](m)}}$ is a matrix we'll denote by $G^{[L](m)}$, whose elements are \n",
    "\\begin{equation}\n",
    "    G^{[L](m)}_{kh} = \\pdv{A_{k}^{[L](m)}}{Z_{h}^{[L](m)}}.\n",
    "\\end{equation}\n",
    "\n",
    "The previous expression then becomes\n",
    "\\begin{equation}\n",
    "    \\grad_{Z^{[L](m)}} L^{(m)} = \\grad_{A^{[L](m)}} L^{(m)}\\, G^{[L](m)},\n",
    "\\end{equation}\n",
    "\n",
    "with\n",
    "\\begin{equation}\\label{eq:grad_z_of_L_index}\n",
    "\\left(\\grad_{Z^{[L](m)}} L^{(m)}\\right)_h = \\sum_{k=1}^{n^{[L]}} \\left(\\grad_{A^{[L](m)}} L^{(m)}\\right)_k \\,G^{[L](m)}_{kh}\\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding $\\grad_{A^{[L](m)}} L^{(m)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here is where the choice of Loss function becomes relevant to the model. And here we are using the cross-entropy error, which yields:\n",
    "\\begin{equation}\n",
    "    \\left(\\grad_{A^{[L](m)}} L^{(m)}\\right)_k = \\pdv{L^{(m)}}{A_k^{[L](m)}} = - \\sum_{q=1}^{n^{[L]}} \\frac{D_q^{(m)}}{A_q^{[L](m)}} \\delta_{kq} = -\\frac{D_k^{(m)}}{A_k^{[L](m)}} = -\\frac{D_k^{(m)}}{Y_k^{(m)}} .\n",
    "\\end{equation}\n",
    "Since the components of $A^{[L](m)}$ are not mixed upon taking the derivative, we can write $\\grad_{A^{[L](m)}} L^{(m)}$ as a Hadamard division, i.e. element-wise division between matrices, denoted by $\\oslash$. But to keep our derivatives layout, we need also do transpose such division, yielding\n",
    "\\begin{equation}\n",
    "    \\grad_{A^{[L](m)}} L^{(m)} = -\\Bigg(D^{(m)}\\oslash A^{[L](m)}\\Bigg)^T.\n",
    "\\end{equation}\n",
    "So far we have:\n",
    "\\begin{equation}\n",
    "    \\grad_{Z^{[L](m)}} J = -\\frac{1}{M}\\Bigg(D^{(m)}\\oslash A^{[L](m)} \\Bigg)^T\\, G^{[L](m)}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding $G^{[L](m)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, let's find $G$. We already know that its elements are\n",
    "\\begin{equation}\n",
    "    G^{[L](m)}_{kh} = \\pdv{A_{k}^{[L](m)}}{Z_{h}^{[L](m)}},\n",
    "\\end{equation}\n",
    "and we compute them by recalling that\n",
    "\\begin{equation}\n",
    "    A_k^{[L](m)} = \\Bigg[g^{[L]}(Z^{[L](m)})\\Bigg]_k.\n",
    "\\end{equation}\n",
    "But now we can't simply compute each component separetely because the last activation function, i.e. $g^{[L]}$, may mix the components of $Z^{[L](m)}$. Here we are using the softmax function, which, indeed, mix the components of $A^{[L](m)}$:\n",
    "\\begin{equation}\n",
    "    \\softmax{Z^{[L](m)}}_k = \\frac{\\exp{Z_k^{[L](m)}}}{\\sum_{r} \\exp{Z_r^{[L](m)}}}.\n",
    "\\end{equation}\n",
    "Let's compute $G^{[L](m)}_{kh}$:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "G^{[L](m)}_{kh} = \\pdv{A_{k}^{[L](m)}}{Z_{h}^{[L](m)}} &= \\frac{\\delta_{kh}\\exp{Z_k^{[L](m)}}}{\\sum_{r} \\exp{Z_r^{[L](m)}}} + \\exp{Z_k^{[L](m)}}\\frac{-1}{\\left(\\sum_{r} \\exp{Z_r^{[L](m)}}\\right)^2}\\sum_{r} \\delta_{hr}\\exp{Z_r^{[L](m)}} \\\\\n",
    "&= \\frac{\\delta_{kh}\\exp{Z_k^{[L](m)}}}{\\sum_{r} \\exp{Z_r^{[L](m)}}} + \\exp{Z_k^{[L](m)}}\\frac{-1}{\\left(\\sum_{r} \\exp{Z_r^{[L](m)}}\\right)^2}\\exp{Z_h^{[L](m)}} \\\\\n",
    "&= \\frac{\\exp{Z_k^{[L](m)}}}{\\sum_{r} \\exp{Z_r^{[L](m)}}}\\delta_{kh} - \\frac{\\exp{Z_k^{[L](m)}}}{\\sum_{r} \\exp{Z_r^{[L](m)}}}\\frac{\\exp{Z_h^{[L](m)}}}{\\sum_{r} \\exp{Z_r^{[L](m)}}},\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "which finally can be written as\n",
    "\\begin{equation}\n",
    "G^{[L](m)}_{kh} = \\softmax{Z^{[L](m)}}_k \\Bigg(\\delta_{kh} - \\softmax{Z^{[L](m)}}_h \\Bigg). \n",
    "\\end{equation}\n",
    "\n",
    "Abbreviating $\\softmax{Z^{[L](m)}}_k$ by $s^{[L](m)}_k$:\n",
    "\\begin{equation}\n",
    "    G^{[L](m)}_{kh} = s^{[L](m)}_k \\bigg(\\delta_{kh} - s^{[L](m)}_h \\bigg). \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to $\\grad_{Z^{[L](m)}} J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with the nice expression:\n",
    "\\begin{equation}\n",
    "    \\grad_{Z^{[L](m)}} J = -\\frac{1}{M}\\Bigg(D^{(m)}\\oslash A^{[L](m)} \\Bigg)^T G^{[L](m)}.\n",
    "\\end{equation}\n",
    "\n",
    "Now that we have a good expression for $\\grad_{Z^{[L](m)}} J$, let's recap what are the dimensions of the terms involved:\n",
    "\n",
    "Term| Belongs to\n",
    "---|---\n",
    "$\\grad_{Z^{[L](m)}} J$ | $\\mat(1, n^{[L]})$\n",
    "$\\Bigg(D^{(m)}\\oslash A^{[L](m)} \\Bigg)^T $ | $\\mat(1, n^{[L]})$\n",
    "$G^{[L](m)}$ | $\\mat(n^{[L]},n^{[L]})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding $\\pdv{J}{W^{[L]}}$ and $\\pdv{J}{b^{[L]}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If now we want to find $\\pdv{J}{W_{ij}^{[L]}}$ or $\\pdv{J}{b_{i}^{[L]}}$ we can use $\\grad_{Z^{[L](m)}}J$:\n",
    "\\begin{equation}\n",
    "    \\pdv{J}{\\set{W_{ij}^{[L]}, b_{i}^{[L]}}} = \\sum_{m=1}^M \\grad_{Z^{[L](m)}} J \\pdv{Z^{[L](m)}}{\\set{W_{ij}^{[L]}, b_{i}^{[L]}}} = \\sum_{m=1}^M\\sum_{h=1}^{n^{[L]}} \\bigg(\\grad_{Z^{[L](m)}}J \\bigg)_h \\pdv{Z_h^{[L](m)}}{\\set{W_{ij}^{[L]}, b_{i}^{[L]}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and since\n",
    "\\begin{equation}\n",
    "    Z_h^{[L](m)} = \\sum_{p=1}^{n^{[L-1]}} W_{hp}^{[L]}A_p^{[L-1](m)} + b_{h}^{[L]},\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have\n",
    "\\begin{equation}\n",
    "    \\pdv{Z_h^{[L](m)}}{W_{ij}^{[L]}} = \\sum_{p=1}^{n^{[L-1]}} \\delta_{ih}\\delta_{jp} A_p^{[L-1](m)} =  \\delta_{ih} A_j^{[L-1](m)},\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and\n",
    "\\begin{equation}\n",
    "    \\pdv{Z_h^{[L](m)}}{b_{i}^{[L]}} =  \\delta_{ih}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we summarize it as\n",
    "\\begin{equation}\n",
    "    \\pdv{Z_h^{[L](m)}}{\\set{W_{ij}^{[L]}, b_{i}^{[L]}}} = \\delta_{ih}\\set{A_j^{[L-1](m)}, 1},\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging this in the expression for $\\pdv{Z_h^{[L](m)}}{\\set{W_{ij}^{[L]}, b_{i}^{[L]}}}$ we get\n",
    "\\begin{equation}\n",
    "    \\pdv{J}{\\set{W_{ij}^{[L]}, b_{i}^{[L]}}} = \\sum_{m=1}^M \\sum_{h=1}^{n^{[L]}} \\bigg(\\grad_{Z^{[L](m)}}J \\bigg)_h \\delta_{ih} \\set{A_j^{[L-1](m)}, 1} ,\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and due to that Dirac's delta $\\delta_{ih}$ this becomes\n",
    "\\begin{equation}\n",
    "    \\pdv{J}{\\set{W_{ij}^{[L]}, b_{i}^{[L]}}} = \\sum_{m=1}^M \\bigg(\\grad_{Z^{[L](m)}}J \\bigg)_i \\set{A_j^{[L-1](m)}, 1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cool thing is that $\\pdv{J}{W_{ij}^{[L]}}$ is the the $(j,i)$ (the order is reversed since we're using the numerator layout) element of the matrix $\\pdv{J}{W^{[L]}}$, and due to the neat form of $\\pdv{J}{W_{ij}^{[L]}}$ we can write\n",
    "\\begin{equation}\n",
    "    \\pdv{J}{\\set{W^{[L]}, b^{[L]}}} = \\sum_{m=1}^M \\set{A^{[L-1](m)},1} \\grad_{Z^{[L](m)}}J \\,  ,\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can get rid of that sum by stacking the $M$ vectors $\\grad_{Z^{[L](m)}}J$ into a matrix, and the same for $A^{[L-1](m)}$:\n",
    "\\begin{equation}\n",
    "    \\pdv{J}{\\set{W^{[L]}, b^{[L]}}} = \\set{A^{[L-1]}, 1^T_{M}} \\big(\\grad_{Z^{[L]}}J \\big)\\, ,\n",
    "\\end{equation}\n",
    "where $1_{M}$ stands for a column vector of ones with size $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's recap:\n",
    "\n",
    "Term | Belongs to\n",
    "---|---\n",
    "$\\pdv{J}{W^{[L]}}$ | $\\mat(n^{[L-1]}, n^{[L]})$\n",
    "$\\pdv{J}{b^{[L]}}$ | $\\mat(1, n^{[L]})$\n",
    "$\\grad_{Z^{[L]}}J $ | $\\mat(M, n^{[L]})$\n",
    "$A^{[L-1]}$|$\\mat(n^{[L-1]}, M)$\n",
    "$1^T_M$|$\\mat(1,M)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding $\\pdv{J}{W^{[L-1]}}$ and $\\pdv{J}{b^{[L-1]}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If now we want to find $\\pdv{J}{W_{ij}^{[L-1]}}$ or $\\pdv{J}{b_{i}^{[L-1]}}$ we can again use $\\grad_{Z^{[L](m)}}J$:\n",
    "\\begin{equation}\n",
    "    \\pdv{J}{\\set{W_{ij}^{[L-1]}, b_{i}^{[L-1]}}} = \\sum_{m=1}^M \\grad_{Z^{[L](m)}} J \\pdv{Z^{[L](m)}}{\\set{W_{ij}^{[L-1]}, b_{i}^{[L-1]}}},\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all we need to do is compute\n",
    "\\begin{equation}\n",
    "    \\pdv{Z^{[L](m)}}{\\set{W_{ij}^{[L-1]}, b_{i}^{[L-1]}}},\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which, by the chain rule, is\n",
    "\\begin{equation}\n",
    "    \\pdv{Z^{[L](m)}}{\\set{W_{ij}^{[L-1]}, b_{i}^{[L-1]}}} = \\pdv{Z^{[L](m)}}{A^{[L-1](m)}} \\pdv{A^{[L-1](m)}}{Z^{[L-1](m)}} \\pdv{Z^{[L-1](m)}}{\\set{W_{ij}^{[L-1]}, b_{i}^{[L-1]}}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term is easy. Since $Z^{[L](m)} = W^{[L]}A^{[L-1](m)} + b^{[L]}$, it follows that\n",
    "\\begin{equation}\n",
    "    \\pdv{Z^{[L](m)}}{A^{[L-1](m)}} = W^{[L]}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second term is, by definition, $G^{[L-1](m)}$, whose components we already know to be\n",
    "\\begin{equation}\n",
    "    G^{[L-1](m)}_{kh} = s^{[L](m)}_k \\bigg(\\delta_{kh} - s^{[L](m)}_h \\bigg). \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last term is analogous to the one we computed last section, thus\n",
    "\\begin{equation}\n",
    "    \\pdv{Z_h^{[L-1](m)}}{\\set{W_{ij}^{[L-1]}, b_{i}^{[L-1]}}} = \\delta_{ih}\\set{A_j^{[L-2](m)}, 1},\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": true,
   "latex_user_defs": true,
   "report_style_numbering": true,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "225px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
